# -*- coding: utf-8 -*-
"""MistralAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18QNiyffyxr4DvoAFN4a4aq5cpgoam7TX

# Mistral LLM Overview

Demonstrates various natural language processing tasks using the Mistral-7B-Instruct-v0.2 model from Hugging Face and the LangChain library.

The notebook covers the following topics:

1.  **Setup and Installation**: Installs necessary libraries including `langchain`, `langchain_huggingface`, `langchain_community`, `huggingface_hub`, `transformers`, `accelerate`, and `bitsandbytes`.
2.  **Import Libraries**: Imports required modules from `langchain`, `transformers`, and `torch`.
3.  **Hugging Face Login**: Logs in to the Hugging Face Hub to access models.
4.  **Quantization**: Sets up 4-bit quantization configuration using `BitsAndBytesConfig` to optimize model loading and memory usage.
5.  **Loading Mistral Model**: Loads the `mistralai/Mistral-7B-Instruct-v0.2` model and its tokenizer from Hugging Face, applying the defined quantization configuration.
6.  **Pipeline and LLM Initialization**: Creates a text generation pipeline using the loaded model and tokenizer, and initializes a LangChain `HuggingFacePipeline` object.
7.  **Example - Question Answering**: Demonstrates a simple question-answering task using a prompt template and the initialized LLM.
8.  **Summarization**: Shows how to summarize a long text using a prompt template and the LLM.
9.  **Multi-Language Support - Translation**: Provides a function and examples for translating text to different languages using the LLM.
10. **Sentiment Analysis**: Presents a function and examples for analyzing the sentiment (positive, negative, or neutral) of text using the LLM.
11. **Topic Modeling**: Illustrates how to identify the main topics in a given text using the LLM.
12. **Multilevel Prompting**: Demonstrates chaining prompts together to perform a task in multiple steps, such as extracting key points and then summarizing them.
13. **Generating Text**: Provides examples of generating creative text from a given prompt using the LLM.
"""

!pip install -q -U langchain langchain_huggingface langchain_community huggingface_hub
!pip install -q -U transformers accelerate bitsandbytes

from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline
from transformers import BitsAndBytesConfig
import torch

from huggingface_hub import notebook_login
notebook_login()

"""**Quantization**"""

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

"""**Loading Mistral Model from Hugging Face**"""

model_id = "mistralai/Mistral-7B-Instruct-v0.2"
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_id)

pipeline_inst = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        use_cache=True,
        device_map="auto",
        max_length=2500,
        do_sample=True,
        top_k=5,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)

llm = HuggingFacePipeline(pipeline=pipeline_inst)

"""## Example"""

template = """<s>[INST] Answer the question below from context below :
{question} [/INST] </s>
"""

def response(question):
  prompt = PromptTemplate.from_template(template)
  chain = prompt | llm
  response = chain.invoke({"question": question})
  return response

response("What is generative AI?")

"""**Summarization**"""

long_text  = """American options are options with an additional right for the holder of the contract.
The option can be exercised at any time prior to or on the day of expiration. An
American option can therefore be worth more than a European option because of
this additional right. The American option can never be worth less than a European
option since the American option will have the same pay as a European option if it
is not exercised prior to the time of expiration, but the additional right of being able
to exercise it early will make it possible to obtain a better pay in some cases.
Assuming that two different portfolios hold one call option each with a stock as the
underlying asset. The strike price is AUD 30 and the current stock price is AUD 30.
The time of expiration is in two months and a discrete dividend will be distributed
after one month. Binomial trees will be used to illustrate how an American option
can have a higher value than a corresponding European option."""

# Summary
prompt = PromptTemplate.from_template("Summarize the following text in 2 sentences:\n{text}")
chain = prompt | llm
summary = chain.invoke({"text": long_text})
print("Summary:")
print(summary)

"""**Multi-Language Support Model: Translation**"""

def translate_text(text, target_language):
  """Translates the given text to the target language using the llm model."""
  prompt = PromptTemplate.from_template(f"Translate the following text to {target_language}:\n{{text}}")
  chain = prompt | llm
  translation = chain.invoke({"text": text})
  return translation

# Example usage:
english_text = "I will go to gym this afternoon."
German_translation = translate_text(english_text, "German")
print(f"English: {english_text}")
print(f"Spanish: {German_translation}")

french_translation = translate_text(english_text, "French")
print(f"English: {english_text}")
print(f"French: {french_translation}")

"""**Sentiment Analysis**"""

def analyze_sentiment(text):
  """Analyzes the sentiment of the given text using the llm model."""
  prompt = PromptTemplate.from_template("What is the sentiment of the following text (positive, negative, or neutral)? \n{text}")
  chain = prompt | llm
  sentiment = chain.invoke({"text": text})
  return sentiment

# Example usage:
text1 = "I love this product! It's amazing."
sentiment1 = analyze_sentiment(text1)
print(f"Text: {text1}")
print(f"Sentiment: {sentiment1}")

text2 = "This is the worst experience I've ever had."
sentiment2 = analyze_sentiment(text2)
print(f"Text: {text2}")
print(f"Sentiment: {sentiment2}")

text3 = "The weather is cloudy today."
sentiment3 = analyze_sentiment(text3)
print(f"Text: {text3}")
print(f"Sentiment: {sentiment3}")

"""**Topic Modelling**"""

def topic_modeling(text):
  """Performs topic modeling on the given text using the llm model."""
  prompt = PromptTemplate.from_template("Identify the main topics in the following text:\n{text}")
  chain = prompt | llm
  topics = chain.invoke({"text": text})
  return topics

# Example usage:
sample_text = """
Artificial intelligence (AI) is a rapidly evolving field that is transforming various aspects of our lives.
Machine learning, a subset of AI, focuses on developing algorithms that allow computers to learn from data without being explicitly programmed.
Deep learning, in turn, is a subfield of machine learning that utilizes artificial neural networks with multiple layers to process complex data such as images and speech.
Natural Language Processing (NLP) is another crucial area of AI that deals with the interaction between computers and human language.
Computer vision enables machines to interpret and understand visual information from the world.
Robotics combines elements of AI, engineering, and computer science to create intelligent machines capable of performing tasks autonomously.
The applications of AI are vast and include areas like healthcare, finance, education, and transportation.
Ethical considerations surrounding AI, such as bias and privacy, are becoming increasingly important as AI systems become more integrated into society.
"""
topics = topic_modeling(sample_text)
print(f"Text:\n{sample_text}")
print(f"Identified Topics: {topics}")

"""**Multilevel Prompting**"""

def multi_level_prompting(text):
  """Demonstrates multi-level prompting using the llm model."""

  # First level: Extract key points
  key_points_prompt = PromptTemplate.from_template("Identify the key points from the following text:\n{text}")
  key_points_chain = key_points_prompt | llm
  key_points = key_points_chain.invoke({"text": text})
  print(f"--- Key Points ---\n{key_points}\n")

  # Second level: Summarize the key points
  summary_prompt = PromptTemplate.from_template("Summarize the following key points:\n{key_points}")
  summary_chain = summary_prompt | llm
  summary = summary_chain.invoke({"key_points": key_points})
  print(f"--- Summary of Key Points ---\n{summary}")

# Example usage:
sample_text_multi = """
The history of artificial intelligence (AI) began in the mid-20th century with the development of early computers and the idea of creating machines that could think.
Key milestones include the Dartmouth Workshop in 1956, often considered the birth of AI as a field, and the development of expert systems in the 1980s.
However, AI research faced periods of reduced funding and interest, known as "AI winters."
The late 20th and early 21st centuries saw a resurgence in AI, driven by advancements in machine learning, increased computing power, and the availability of large datasets.
Deep learning techniques, in particular, have led to significant breakthroughs in areas like image recognition and natural language processing.
Today, AI is being applied in numerous fields, from healthcare and finance to transportation and entertainment, raising important ethical and societal questions.
"""

multi_level_prompting(sample_text_multi)

"""**Generating Text**"""

# Generate text from a prompt
prompt_text = "Write a short story about a robot who wants to be a painter."
generated_text = llm.invoke(prompt_text)
print("Generated Text:")
print(generated_text)

# Generate text from a prompt
prompt_text = "Describe quantum computing in 5 sentences."
generated_text = llm.invoke(prompt_text)
print("Generated Text:")
print(generated_text)

# Generate text from a prompt
prompt_text = "What is generative AI."
generated_text = llm.invoke(prompt_text)
print("Generated Text:")
print(generated_text)

